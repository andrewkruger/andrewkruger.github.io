---
layout: post
title: "Keras and MNIST"
author: andrew
tags: [blog]
description: >
---


The following is a breakdown of [a Keras Convolutional Neural Network](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) that can be used to detect numbers in the MNIST dataset.  First, the dependancies:

~~~py
    from __future__ import print_function
    import keras
    from keras.datasets import mnist
    from keras.models import Sequential
    from keras.layers import Dense, Dropout, Flatten
    from keras.layers import Conv2D, MaxPooling2D
    from keras import backend as K
    from keras.preprocessing.image import ImageDataGenerator
~~~



I used Keras to train a Convolutional Neural Network (CNN) on the MNIST data set on AWS (g2.2xlarge GPU), and changed parameters to see how the training would change.  For each of the following, I did 3 epochs with 60,000 training images and 10,000 test images.


~~~py
    batch_size = 128
    num_classes = 10
    epochs = 3
~~~

Number of classes is 10 because there are 10 numbers (0-9).  Epochs are the number of times we train on the images.  Training more times on the images increases the accuracy because the fit parameters are optimized by gradient descent.  In the first epoch, the fit parameters are far from ideal and they make large changes at first.  After we have trained on all the images and started over, it's the same as starting the initial epoch with more ideal parameters.  So each time we train on the images, it's the just same as doing that first epoch with more optimized parameters.

The batch size is the number of images the CNN is trained on at a time.  For example, if the batch size is 16, it means the CNN will be trained on 16 images and the weights will be updated before it takes another 16 images to train on.  The training is faster on larger batches, but the batch size is limited by the memory size of the GPU.  I tested how much the batch size makes a difference by training on AWS (g2.2xlarge GPU) with 3 epochs.  There were 60,000 training images and 10,000 test images.  It can be seen the test loss and accuracy is relatively unchanged, but larger batches decreased the total time.


| | Epoch 1 | Epoch 2 | Epoch 3 | Test | |
| Batch Size | Loss, Acc. | Loss, Acc. | Loss, Acc. | Loss, Acc. | Total Time (s) |
| ------------- |:-------------:| -----:|-----:|-----:|-----:|
| 16     | 0.2202, 0.9339 | 0.0931, 0.9728 | 0.0744, 0.9783 | 0.04339, 0.9858 | 443 |
| 32     | 0.2385, 0.9272 | 0.0989, 0.9712 | 0.0776, 0.9773 | 0.03952, 0.9864 | 348 |
| 64     | 0.2724, 0.9172 | 0.1041, 0.9693 | 0.0791, 0.9761 | 0.04066, 0.9861 | 330 |




The MNIST images are all 28x28 pixels.  Here we retrieve images and resize them:

~~~py
    # input image dimensions
    img_rows, img_cols = 28, 28

    # the data, shuffled and split between train and test sets
    (x_train, y_train), (x_test, y_test) = mnist.load_data()


    if K.image_data_format() == 'channels_first':
        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
        input_shape = (1, img_rows, img_cols)
    else:
        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
        input_shape = (img_rows, img_cols, 1)
~~~

### Activation Function and Pre-processing

A neuron's "activation" is determined by an *activation function*.  An example *activation function* is a sigmoid function which follows the relationship 

$$
\sigma(x) = \frac{1}{1+exp^{-x}}
$$

where $$x$$ is the dot product of the input and the weights.  When $$x$$ is small, $$\sigma$$ will be closer to zero (neuron is not activated), but a large $$x$$ will make $$\sigma$$ closer to one (neuron gets activated).  So a single neuron acts similar to a linear classifier.

Another common activation function is the Rectified Linear Unit (ReLU) which follows

$$
f(x) = max(0,x)
$$

In this case, if $$x$$ is less than 0, then the *activation function* is just zero.  Otherwise, it increases linearly with $$x$$ and doesn't have a maximum of one.  These characteristics help the model weights converge faster, although there's an added risk that if the learning rate is too high, a weight change could make $$x$$ less than zero.  When this happens, there is a loss of information (can't do gradient descent if $$x$$ is always zero!) that can keep $$x$$ permanently zero, and neurons in the network may be "dead".

The images are on an 8-bit grayscale with 256 intensities (in the range 0-255).  However, using numbers this large can result in *activation saturation*.  Since $$x$$ is the dot product of the input and the weights, if the inputs are larger, the weights should be smaller.  In the case of the sigmoid activation function, starting out weights that aren't small results in the sigmoid will be forced to be one for all inputs.  Thus some weights may not be corrected with those neurons always being activated.  Likewise with ReLU, the $$x$$ can be a high value regardless of the weight.  When this model was fit to un-normalized data, For this reason, the images are normalized so they are in the range 0-1 instead of 0-255 by dividing by 255.  


~~~py
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255
    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')
    print(x_test.shape[0], 'test samples')
~~~


