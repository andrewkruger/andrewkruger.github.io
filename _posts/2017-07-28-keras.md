---
layout: post
title: "Keras and MNIST"
author: andrew
tags: [blog]
description: >
---


I used Keras to train a Convolutional Neural Network (CNN) on the MNIST data set on AWS (g2.2xlarge GPU), and changed parameters to see how the training would change.  For each of the following, I did 3 epochs with 60,000 training images and 10,000 test images.


#### Batch Size

The batch size is the number of images the CNN is trained on at a time.  For example, if the batch size is 16, it means the CNN will be trained on 16 images and the weights will be updated before it takes another 16 images to train on.  The training is faster on larger batches, but the batch size is limited by the memory size of the GPU.  I wanted to test how training changes for different batch sizes, and found the following losses and accuracies in the 3 epochs with the different batch sizes.  It can be seen the test loss and accuracy is relatively unchnaged, but larger batches decreased the total time.


| | Epoch 1 | Epoch 2 | Epoch 3 | Test | |
| Batch Size | Loss, Acc. | Loss, Acc. | Loss, Acc. | Loss, Acc. | Total Time (s) |
| ------------- |:-------------:| -----:|-----:|-----:|-----:|
| 16     | 0.2202, 0.9339 | 0.0931, 0.9728 | 0.0744, 0.9783 | 0.04339, 0.9858 | 443 |
| 32     | 0.2385, 0.9272 | 0.0989, 0.9712 | 0.0776, 0.9773 | 0.03952, 0.9864 | 348 |
| 64     | 0.2724, 0.9172 | 0.1041, 0.9693 | 0.0791, 0.9761 | 0.04066, 0.9861 | 330 |


